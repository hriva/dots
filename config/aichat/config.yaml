# see https://github.com/sigoden/aichat/blob/main/config.example.yaml

model: ollama:deepseek-r1:7b
# ollama defaults
temperature: 0.0
top_p: 0.9
# 
function_calling: true # Enables or disables function calling (Globally).
use_tools: null
save_session: true
compress_threshold: 8192
clients:
  - type: openai-compatible
    name: ollama
    api_base: null
    api_key: null
    models:
      - name: llama3.1:latest
        max_input_tokens: 128000
        supports_function_calling: true
      - name: qwen2.5-coder:latest
        max_input_tokens: 1048576
        supports_function_calling: true
      - name: qwen2.5-coder:3b
        max_input_tokens: 8192
      - name: deepseek-r1:7b
        max_input_tokens: 131072
        supports_function_calling: true
      - name: llama3.2-vision:latest
        max_input_tokens: 131072
        supports_function_calling: true
        supports_vision: true
    patch:
      chat_completions:
        deepseek-r1.*:
          body:
            options:
              top_k: 60,
              top_p: 0.9,
              temperature: 0.6
  - type: gemini
    models:
      - name: gemini-2.0-flash-thinking-exp-01-21
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_price: 0.35
        output_price: 0.53
        supports_vision: true
      - name: gemini-2.0-flash
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_price: 0.35
        output_price: 0.53
        supports_vision: true
      - name: gemini-1.5-flash-latest
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_price: 0.35
        output_price: 0.53
        supports_vision: true
      - name: gemini-1.5-flash-8b-latest
        max_input_tokens: 1048576
        max_output_tokens: 8192
        input_price: 0.35
        output_price: 0.53
        supports_vision: true
  - type: claude
editor: vi
